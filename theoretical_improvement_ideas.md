- Use Local complexity as loss.  
- Use Local complexity as input for a scheduler.  
- Use adv attacks as probes to compute local complexity.  
- Link adv attacks, local complexity and loss sharpness.  
- Use loss (first and/or 2nd order)gradients amplitude as a regularization loss and maybe learning rate scheduling.  
- use loss magnitude as a loss in itself for regularization.
- LLMs are pretrained on vast datasets, then through RLVR they don't actually learn more but rather reinforce good/correct concepts already learned during pretraining. => Use mechanistic intepretabillity to find what are the bad and good samples in pretraining and clean pretraining dataset.
  
